pyspark 

gremlin00:~/spark761> hdfs dfs -put s.txt

>>> text_RDD = sc.textFile("s.txt")

>>> text_RDD.take(1) 

>>> def split_words(line): return line.split()

>>> def create_pair(word): return (word, 1)

>>> pairs_RDD=text_RDD.flatMap(split_words).map(create_pair) 

>>> pairs_RDD.collect()

>>> def sum_counts(a, b): return a + b 

>>> wordcounts_RDD = pairs_RDD.reduceByKey(sum_counts) 

>>> wordcounts_RDD.collect() 



